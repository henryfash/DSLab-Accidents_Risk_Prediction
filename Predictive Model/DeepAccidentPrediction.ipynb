{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def reshape_by_catagory(array, category, SEQ=2):\n",
    "    # l=[]\n",
    "    # if category!='geohash' and  category!='NLP' :\n",
    "    #     b = array[0:-14]\n",
    "    #     for i in range(SEQ):\n",
    "    #         c = b[i*25:i*25+25]\n",
    "    #         if category == 'traffic':\n",
    "    #             #d = np.concatenate((c[0:9],c[-5:]),axis=1)\n",
    "    #             d = np.concatenate([c[1:2],c[3:10]],axis=1)\n",
    "    #         elif category=='weather':\n",
    "    #             d = c[10:-5]\n",
    "    #         elif category=='time':\n",
    "    #             d = np.concatenate([c[0:1],c[2:3],c[-5:]],axis=1)\n",
    "    #         else:\n",
    "    #             d = c\n",
    "    #         l.append(d)\n",
    "    #     n = np.concatenate(l,axis=1)\n",
    "    #     #if category!='no_geohash':\n",
    "    #     #    return np.concatenate((n,array[-14:]),axis=1)\n",
    "    #     return n\n",
    "    if category=='NLP':\n",
    "        return array[-100:]\n",
    "    elif category=='TimeVariant':\n",
    "        array = array[0:-114]\n",
    "        return array.reshape((SEQ,int(array.shape[0]/SEQ)))\n",
    "    else:\n",
    "        return array[-114:-100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class AccidentDataset(data.Dataset):\n",
    "    def __init__(self, X_npy_files: [str], y_npy_files: [str]):\n",
    "        self.X_data = np.concatenate([np.load(f, allow_pickle=True) for f in X_npy_files], axis=0)\n",
    "        self.y_data = np.concatenate([np.load(f, allow_pickle=True) for f in y_npy_files], axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        elemX = self.X_data[item, :-1]\n",
    "        geo_code = self.X_data[item, -1]\n",
    "        time_variant = reshape_by_catagory(elemX, \"TimeVariant\", SEQ=8)\n",
    "        geohash2vec = reshape_by_catagory(elemX, \"NLP\")\n",
    "        poi = reshape_by_catagory(elemX, \"geohash\")\n",
    "        elemY = self.y_data[item]\n",
    "\n",
    "        # print('shapes')\n",
    "        # print('elemX', elemX.shape)\n",
    "        # print('time_variant', time_variant.shape)\n",
    "        # print('desc2vec', desc2vec.shape)\n",
    "        # print('poi', poi.shape)\n",
    "        # print('geocode', geo_code)\n",
    "        # sys.exit()\n",
    "\n",
    "        sample = {\n",
    "            'time_variant': torch.tensor(time_variant.astype(np.float), dtype=torch.float),\n",
    "            'geohash2vec': torch.tensor(geohash2vec.astype(np.float), dtype=torch.float),\n",
    "            'poi': torch.tensor(poi.astype(np.float), dtype=torch.float),\n",
    "            'geo_code': torch.tensor(geo_code),\n",
    "            'y': torch.tensor(elemY)\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class DeepAccidentPrediction(pl.LightningModule):\n",
    "    def __init__(self, cities: [str], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cities = cities\n",
    "        self.time_variant_component = nn.LSTM(input_size=25, hidden_size=128, num_layers=2)\n",
    "        self.embedding_component = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=935, embedding_dim=128),\n",
    "            nn.Linear(128, 128),\n",
    "            )\n",
    "        self.poi_component = nn.Linear(14, 128)\n",
    "        self.geohash2vec = nn.Linear(100, 128)\n",
    "        self.final_dense_layers =  nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, tv, g2v, poi, gcode):\n",
    "        out, _ = self.time_variant_component(tv)\n",
    "        tv = out[:, -1, :]\n",
    "        tv = torch.relu(tv)\n",
    "\n",
    "        gcode_vector = self.embedding_component(gcode)\n",
    "        gcode_vector = torch.relu(gcode_vector)\n",
    "\n",
    "        g2v = self.geohash2vec(g2v)\n",
    "        g2v = torch.relu(g2v)\n",
    "\n",
    "        poi = self.poi_component(poi)\n",
    "        poi = torch.relu(poi)\n",
    "\n",
    "        fc_inp = torch.cat((tv, g2v, poi, gcode_vector), dim=1)\n",
    "        return self.final_dense_layers(fc_inp)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        return [optimizer]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = AccidentDataset(\n",
    "            X_npy_files=[f\"../data_files/dataset/X_train_{city}.npy\" for city in self.cities],\n",
    "            y_npy_files=[f\"../data_files/dataset/y_train_{city}.npy\" for city in self.cities],\n",
    "        )\n",
    "        dataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = AccidentDataset(\n",
    "            X_npy_files=[f\"../data_files/dataset/X_test_{city}.npy\" for city in self.cities],\n",
    "            y_npy_files=[f\"../data_files/dataset/y_test_{city}.npy\" for city in self.cities],\n",
    "        )\n",
    "        dataloader = data.DataLoader(dataset, batch_size=128, num_workers=4, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        out =  self.forward(tv=batch['time_variant'], poi=batch['poi'], g2v=batch['geohash2vec'], gcode=batch['geo_code'])\n",
    "        loss = self.loss(out, batch['y'])\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        out =  self.forward(tv=batch['time_variant'], poi=batch['poi'], g2v=batch['geohash2vec'], gcode=batch['geo_code'])\n",
    "        acc =  (torch.argmax(out, dim=1) == batch['y']).count_nonzero() / (len(batch['y']) * 1.0)\n",
    "        loss = self.loss(out, batch['y'])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"val_batch_loss\": loss, \"val_batch_acc\": acc}\n",
    "\n",
    "    def on_train_end(self):\n",
    "        y_pred, y_true = self.predict(return_labels=True)\n",
    "        print(\"Train Stopped: Printing F1 Score (of Validation) ...\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, return_labels=True):\n",
    "        dataloader = self.val_dataloader()\n",
    "        predY: [int] = []\n",
    "        actualY: [int] = []\n",
    "        for batch in dataloader:\n",
    "            tv, poi, geohash2vec, geo_code, y = batch['time_variant'].to(self.device), \\\n",
    "                                             batch['poi'].to(self.device), \\\n",
    "                                             batch['geohash2vec'].to(self.device), \\\n",
    "                                             batch['geo_code'].to(self.device), \\\n",
    "                                             batch['y'].to(self.device)\n",
    "\n",
    "\n",
    "            y_hat = self.forward(tv=tv, poi=poi, g2v=geohash2vec, gcode=geo_code)\n",
    "            class_predictions = torch.argmax(y_hat, dim=1)\n",
    "            predY.extend(class_predictions.tolist())\n",
    "            actualY.extend(y.tolist())\n",
    "\n",
    "        if return_labels:\n",
    "            return predY, actualY\n",
    "\n",
    "        return predY"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ...\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     26137\n",
      "           1       0.68      0.37      0.48      5341\n",
      "\n",
      "    accuracy                           0.86     31478\n",
      "   macro avg       0.78      0.67      0.70     31478\n",
      "weighted avg       0.85      0.86      0.85     31478\n",
      "\n",
      "Completed training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training ...\")\n",
    "model = DeepAccidentPrediction(cities=[\"Atlanta\", \"Austin\", \"Charlotte\", \"Dallas\", \"Houston\", \"LosAngeles\"])\n",
    "trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            num_nodes=1,\n",
    "            deterministic=True,\n",
    "            max_epochs=10,\n",
    "            progress_bar_refresh_rate=0, # comment to enable progress bar\n",
    "        )\n",
    "trainer.fit(model)\n",
    "print(\"Completed training!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type             | Params\n",
      "------------------------------------------------------------\n",
      "0 | time_variant_component | LSTM             | 211 K \n",
      "1 | embedding_component    | Sequential       | 136 K \n",
      "2 | poi_component          | Linear           | 1.9 K \n",
      "3 | geohash2vec            | Linear           | 12.9 K\n",
      "4 | final_dense_layers     | Sequential       | 147 K \n",
      "5 | loss                   | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ... Dataset: Atlanta\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91      1984\n",
      "           1       0.69      0.49      0.58       531\n",
      "\n",
      "    accuracy                           0.85      2515\n",
      "   macro avg       0.78      0.72      0.74      2515\n",
      "weighted avg       0.84      0.85      0.84      2515\n",
      "\n",
      "Begin training ... Dataset: Austin\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      3863\n",
      "           1       0.69      0.55      0.61       801\n",
      "\n",
      "    accuracy                           0.88      4664\n",
      "   macro avg       0.80      0.75      0.77      4664\n",
      "weighted avg       0.87      0.88      0.87      4664\n",
      "\n",
      "Begin training ... Dataset: Charlotte\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91      3446\n",
      "           1       0.71      0.49      0.58       939\n",
      "\n",
      "    accuracy                           0.85      4385\n",
      "   macro avg       0.79      0.72      0.74      4385\n",
      "weighted avg       0.84      0.85      0.84      4385\n",
      "\n",
      "Begin training ... Dataset: Dallas\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      4723\n",
      "           1       0.61      0.15      0.24       622\n",
      "\n",
      "    accuracy                           0.89      5345\n",
      "   macro avg       0.75      0.57      0.59      5345\n",
      "weighted avg       0.86      0.89      0.86      5345\n",
      "\n",
      "Begin training ... Dataset: Houston\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      7326\n",
      "           1       0.67      0.39      0.50      1111\n",
      "\n",
      "    accuracy                           0.89      8437\n",
      "   macro avg       0.79      0.68      0.72      8437\n",
      "weighted avg       0.88      0.89      0.88      8437\n",
      "\n",
      "Begin training ... Dataset: LosAngeles\n",
      "Train Stopped: Printing F1 Score (of Validation) ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88      4795\n",
      "           1       0.61      0.30      0.40      1337\n",
      "\n",
      "    accuracy                           0.80      6132\n",
      "   macro avg       0.72      0.62      0.64      6132\n",
      "weighted avg       0.78      0.80      0.78      6132\n",
      "\n",
      "Completed training!\n"
     ]
    }
   ],
   "source": [
    "for city in [\"Atlanta\", \"Austin\", \"Charlotte\", \"Dallas\", \"Houston\", \"LosAngeles\"]:\n",
    "    print(f\"Begin training ... Dataset: {city}\")\n",
    "    model = DeepAccidentPrediction(cities=[city])\n",
    "    trainer = pl.Trainer(\n",
    "                gpus=1,\n",
    "                num_nodes=1,\n",
    "                deterministic=True,\n",
    "                max_epochs=10,\n",
    "                progress_bar_refresh_rate=0, # comment to enable progress bar\n",
    "            )\n",
    "    trainer.fit(model)\n",
    "print(\"Completed training!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}